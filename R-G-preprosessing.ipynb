{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "817bb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = r\"C:\\Users\\SuZamii\\Desktop\\c441\\Recipe-Gen final-project\\data\\RAW_recipes.csv\"\n",
    "recipes_df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac1d925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 231637 rows and 12 columns.\n"
     ]
    }
   ],
   "source": [
    "# Get the number of rows and columns\n",
    "num_rows, num_columns = recipes_df.shape\n",
    "print(f\"The dataset contains {num_rows} rows and {num_columns} columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e30456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The filtered dataset contains 212401 rows and 12 columns.\n"
     ]
    }
   ],
   "source": [
    "# Define a list of common ingredients\n",
    "common_ingredients = ['tomato', 'egg', 'potato', 'onion', 'garlic', 'butter', 'milk', 'cheese', 'flour', 'sugar']\n",
    "\n",
    "# Convert ingredients to lowercase and check for common ingredients\n",
    "filtered_df = recipes_df[recipes_df['ingredients'].str.lower().apply(lambda x: any(ing in x for ing in common_ingredients))]\n",
    "# Check the number of rows in the filtered dataset\n",
    "print(f\"The filtered dataset contains {filtered_df.shape[0]} rows and {filtered_df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1338493a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The filtered dataset contains 178935 rows and 12 columns.\n"
     ]
    }
   ],
   "source": [
    "# Filter for recipes with 1 <= minutes < 200 and fewer than 15 ingredients\n",
    "filtered_df = filtered_df[(filtered_df['n_ingredients'] < 15) & (filtered_df['minutes'] > 1) & (filtered_df['n_steps'] > 1) & (filtered_df['minutes'] < 200)]\n",
    "\n",
    "# Check the number of rows in the filtered dataset\n",
    "print(f\"The filtered dataset contains {filtered_df.shape[0]} rows and {filtered_df.shape[1]} columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78eb0066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final sampled dataset contains 100000 rows and 12 columns.\n"
     ]
    }
   ],
   "source": [
    "# Take a random sample of 100,000 recipes from the filtered dataset\n",
    "filtered_df = filtered_df.sample(100000, random_state=1)\n",
    "\n",
    "# Check the final dataset size\n",
    "print(f\"The final sampled dataset contains {filtered_df.shape[0]} rows and {filtered_df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5ea3959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining columns: Index(['name', 'minutes', 'n_steps', 'steps', 'description', 'ingredients',\n",
      "       'n_ingredients'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "filtered_df = filtered_df.drop(columns=['id', 'contributor_id', 'submitted', 'tags', 'nutrition'])\n",
    "\n",
    "# Check the remaining columns to confirm\n",
    "print(\"Remaining columns:\", filtered_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "307f1e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final sampled dataset contains 100000 rows and 7 columns.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The final sampled dataset contains {filtered_df.shape[0]} rows and {filtered_df.shape[1]} columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdb3e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered DataFrame to a CSV file\n",
    "filtered_df.to_csv(\"filtered_recipes.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82907cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of unique random IDs between 1 and 100,000\n",
    "num_rows = filtered_df.shape[0]\n",
    "random_ids = random.sample(range(1, 100001), num_rows)\n",
    "\n",
    "# Assign the random IDs to a new 'id' column\n",
    "filtered_df['id'] = random_ids\n",
    "\n",
    "# Move 'id' to the first column\n",
    "cols = ['id'] + [col for col in filtered_df.columns if col != 'id']\n",
    "filtered_df = filtered_df[cols]\n",
    "\n",
    "# Check the first few rows to confirm 'id' is the first column\n",
    "# print(filtered_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a2adc",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93c0aa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique ingredients: 11522\n"
     ]
    }
   ],
   "source": [
    "# Convert each ingredient list into a list of ingredients if it's a string\n",
    "def convert_to_list(ingredient_str):\n",
    "    if isinstance(ingredient_str, str):\n",
    "        try:\n",
    "            return eval(ingredient_str)  # Convert string to list\n",
    "        except:\n",
    "            return []  # Return an empty list if eval fails\n",
    "    else:\n",
    "        return ingredient_str  # Return as is if already a list or non-string\n",
    "\n",
    "filtered_df['ingredients'] = filtered_df['ingredients'].apply(convert_to_list)\n",
    "\n",
    "# Flatten the list of ingredients across all recipes\n",
    "flattened_ingredients = [ingredient.lower().strip() for ingredients in filtered_df['ingredients'] for ingredient in ingredients]\n",
    "\n",
    "# Count occurrences of each ingredient\n",
    "ingredient_counts = Counter(flattened_ingredients)\n",
    "\n",
    "# Display the ingredient counts\n",
    "# print(ingredient_counts)\n",
    "\n",
    "# Calculate the total number of unique ingredients\n",
    "total_unique_ingredients = len(ingredient_counts)\n",
    "\n",
    "print(f\"Total unique ingredients: {total_unique_ingredients}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a7007f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattened list of unique ingredients\n",
    "unique_ingredients = list(ingredient_counts.keys())\n",
    "\n",
    "# Define a set of stopwords to ignore\n",
    "stopwords = {'fresh', 'chopped', 'sliced', 'diced', 'organic', 'crushed', 'small', 'large', 'and', 'dried', \n",
    "             'free', 'mix', 'low', 'hot', 'of', 'with', 'red', 'white', 'black', 'brown', 'yellow', 'fat', \n",
    "             'light', 'style', 'in', 'lean', 'dark', 'french', 'smoked', 'roast'}\n",
    "\n",
    "# Create a unique token dictionary with numeric identifiers\n",
    "token_dict = {}\n",
    "token_id = 1\n",
    "\n",
    "# Prepare data for each ingredient\n",
    "data = []\n",
    "for ingredient in unique_ingredients:\n",
    "    # Tokenize the ingredient, removing punctuation\n",
    "    ingredient_tokens = re.findall(r'\\b\\w+\\b', ingredient.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    ingredient_tokens = [token for token in ingredient_tokens if token not in stopwords]\n",
    "    \n",
    "    # Convert tokens to numeric identifiers\n",
    "    token_ids = []\n",
    "    for token in ingredient_tokens:\n",
    "        if token not in token_dict:\n",
    "            token_dict[token] = token_id\n",
    "            token_id += 1\n",
    "        token_ids.append(token_dict[token])\n",
    "\n",
    "    # Get the count of the ingredient from ingredient_counts\n",
    "    count = ingredient_counts[ingredient]\n",
    "    \n",
    "    # Append ingredient, token IDs, and count to the data list\n",
    "    data.append({\n",
    "        'ingredient': ingredient,\n",
    "        'token_ids': ', '.join(map(str, token_ids)),  # Join token IDs as a string for easy viewing\n",
    "        'count': count\n",
    "    })\n",
    "\n",
    "# Convert data to a DataFrame\n",
    "tokenized_ingredient_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Save the token dictionary as well for reference\n",
    "token_df = pd.DataFrame(list(token_dict.items()), columns=['token', 'token_id'])\n",
    "\n",
    "\n",
    "\n",
    "# print(tokenized_ingredient_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7aa30387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              ingredients  \\\n",
      "181397  [butter, olive oil, scallion, mushrooms, seaso...   \n",
      "77568   [margarine, brown sugar, pecans, graham cracke...   \n",
      "28270   [chicken breasts, onions, red bell peppers, mu...   \n",
      "206145  [mixed baby lettuces and spring greens, pear, ...   \n",
      "83001   [white beans, extra virgin olive oil, fresh le...   \n",
      "\n",
      "                                    tokenized_ingredients  \n",
      "181397  [[1], [2, 3], [4], [5], [6, 7], [8, 9], [10, 1...  \n",
      "77568                    [[14], [15], [16], [17, 18, 19]]  \n",
      "28270   [[20, 21], [22], [23, 24], [5], [25], [26], [2...  \n",
      "206145  [[30, 31, 32, 33, 34], [35], [36, 20], [37, 38...  \n",
      "83001   [[48], [49, 50, 2, 3], [51, 43], [52, 53], [7]...  \n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize an ingredient list based on token_dict\n",
    "def tokenize_ingredient_list(ingredient_list):\n",
    "    tokenized_list = []\n",
    "    for ingredient in ingredient_list:\n",
    "        # Tokenize and clean the ingredient\n",
    "        ingredient_tokens = re.findall(r'\\b\\w+\\b', ingredient.lower())\n",
    "        ingredient_tokens = [token for token in ingredient_tokens if token not in stopwords]\n",
    "        \n",
    "        # Convert tokens to numeric token IDs based on token_dict\n",
    "        token_ids = [token_dict[token] for token in ingredient_tokens if token in token_dict]\n",
    "        tokenized_list.append(token_ids)\n",
    "    \n",
    "    return tokenized_list\n",
    "\n",
    "# Apply the tokenization function to each row in filtered_df and rename to filtered_df_tokenized\n",
    "filtered_df_tokenized = filtered_df.copy()  # Copy to avoid modifying the original DataFrame\n",
    "filtered_df_tokenized['tokenized_ingredients'] = filtered_df_tokenized['ingredients'].apply(tokenize_ingredient_list)\n",
    "\n",
    "# Display the updated DataFrame with the new 'tokenized_ingredients' column\n",
    "print(filtered_df_tokenized[['ingredients', 'tokenized_ingredients']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0947604f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of            id  minutes  n_steps  n_ingredients  \\\n",
      "181397  76104       45        6              8   \n",
      "77568   40098       25        8              4   \n",
      "28270   20898       75       11              9   \n",
      "206145   8643       20        4             14   \n",
      "83001   54357       10        5             10   \n",
      "...       ...      ...      ...            ...   \n",
      "168444  59432       20        4              5   \n",
      "227081  87429      155       17              9   \n",
      "186504  48485       15        6              6   \n",
      "188200  71769       35        9             11   \n",
      "83432   38504       70       12              9   \n",
      "\n",
      "                                    tokenized_ingredients  \n",
      "181397  [[1], [2, 3], [4], [5], [6, 7], [8, 9], [10, 1...  \n",
      "77568                    [[14], [15], [16], [17, 18, 19]]  \n",
      "28270   [[20, 21], [22], [23, 24], [5], [25], [26], [2...  \n",
      "206145  [[30, 31, 32, 33, 34], [35], [36, 20], [37, 38...  \n",
      "83001   [[48], [49, 50, 2, 3], [51, 43], [52, 53], [7]...  \n",
      "...                                                   ...  \n",
      "168444  [[20, 21], [127], [111, 112], [122, 3], [87, 22]]  \n",
      "227081  [[422, 64], [139, 3], [76, 77, 75], [75], [15]...  \n",
      "186504  [[49, 50, 2, 3], [102, 43], [447, 43], [91], [...  \n",
      "188200  [[124], [2, 3], [254, 48], [266], [87, 128], [...  \n",
      "83432   [[9], [9], [87, 9], [49, 50, 2, 3], [91, 114],...  \n",
      "\n",
      "[100000 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Drop the specified columns from the DataFrame\n",
    "filtered_df_tokenized = filtered_df_tokenized.drop(columns=['name', 'steps', 'description', 'ingredients'])\n",
    "print(filtered_df_tokenized.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b37e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered DataFrame to a CSV file\n",
    "filtered_df_tokenized.to_csv(\"tokenized_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb7e5c9",
   "metadata": {},
   "source": [
    "# Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0106c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(token_dict) + 1  # +1 for padding index if needed\n",
    "embedding_dim = 100  # Dimension of each embedding vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7b836",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create the Embedding Layer in Your Model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding, LSTM, Dense\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m      7\u001b[0m     Embedding(input_dim\u001b[38;5;241m=\u001b[39mvocab_size, output_dim\u001b[38;5;241m=\u001b[39membedding_dim, input_length\u001b[38;5;241m=\u001b[39mmax_length),\n\u001b[0;32m      8\u001b[0m     LSTM(\u001b[38;5;241m128\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),  \u001b[38;5;66;03m# LSTM or any other sequence model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     Dense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     10\u001b[0m     Dense(num_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# or linear for regression\u001b[39;00m\n\u001b[0;32m     11\u001b[0m ])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# # Create the Embedding Layer in Your Model\n",
    "\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# model = Sequential([\n",
    "#     Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "#     LSTM(128, return_sequences=True),  # LSTM or any other sequence model\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(num_classes, activation='softmax')  # or linear for regression\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d339f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare Your Data\n",
    "\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# # Assuming tokenized_ingredient_lists contains token IDs for each recipe\n",
    "# max_length = max(len(x) for x in tokenized_ingredient_lists)  # Define the max length of sequences\n",
    "# padded_sequences = pad_sequences(tokenized_ingredient_lists, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cf4191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Train the Model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(padded_sequences, labels, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b4cbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305176e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a637c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67152c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e34cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f98f43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7770f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2f933a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bdfa452",
   "metadata": {},
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b1c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "filtered_df.to_csv('ingredients_with_token_ids.csv', index=False)\n",
    "token_df.to_csv('token_reference.csv', index=False)\n",
    "print(\"Data saved to ingredients_with_token_ids.csv and token_reference.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39fd0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattened list of unique ingredients\n",
    "unique_ingredients = list(ingredient_counts.keys())\n",
    "\n",
    "# Define a set of stopwords to ignore (add more as needed)\n",
    "stopwords = {'fresh', 'chopped', 'sliced', 'diced', 'organic',  'crushed', 'small', 'large', 'and', 'dried', \n",
    "             'free', 'mix', 'low', 'hot', 'of', 'with','red','white','black','brown','yellow','fat','light' ,\n",
    "              'style', 'in' , 'lean', 'dark', 'french', 'smoked', 'roast'}\n",
    "\n",
    "# Tokenize each ingredient, remove stopwords, and flatten the result\n",
    "tokens = []\n",
    "for ingredient in unique_ingredients:\n",
    "    # Split by spaces and remove punctuation using regex\n",
    "    ingredient_tokens = re.findall(r'\\b\\w+\\b', ingredient.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    ingredient_tokens = [token for token in ingredient_tokens if token not in stopwords]\n",
    "    \n",
    "    # Add tokens to the list\n",
    "    tokens.extend(ingredient_tokens)\n",
    "\n",
    "# Count each token's occurrences\n",
    "token_counts = Counter(tokens)\n",
    "\n",
    "# Display token counts\n",
    "print(f\"Total unique tokens: {len(token_counts)}\")\n",
    "print(\"Most common tokens:\", token_counts.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd63d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate single-word and multi-word ingredients\n",
    "single_word_ingredients = set()  # Using a set to avoid duplicates\n",
    "multi_word_ingredients = []\n",
    "\n",
    "for ingredient in ingredient_counts.keys():\n",
    "    if len(ingredient.split()) == 1:\n",
    "        single_word_ingredients.add(ingredient)\n",
    "    else:\n",
    "        multi_word_ingredients.append(ingredient)\n",
    "\n",
    "# Define common descriptors to identify in multi-word ingredients\n",
    "descriptors = {\"unsalted\", \"salted\", \"fresh\", \"ground\", \"dried\", \"chopped\", \"large\", \"small\"}\n",
    "\n",
    "# Tokenize single-word ingredients directly\n",
    "tokenized_single_word = list(single_word_ingredients)  # Convert to list for consistent output\n",
    "\n",
    "# Tokenize multi-word ingredients by identifying core and descriptors\n",
    "tokenized_multi_word = []\n",
    "for ingredient in multi_word_ingredients:\n",
    "    tokens = ingredient.split()  # Split the ingredient into individual words\n",
    "    # Identify core and descriptor(s)\n",
    "    core = tokens[-1] if tokens[-1] not in descriptors else tokens[-2]  # Assume last word is core if not a descriptor\n",
    "    descriptor = [token for token in tokens if token in descriptors]  # Collect descriptors\n",
    "    \n",
    "    # Only add the core if it's not already in the single-word list\n",
    "    if core not in single_word_ingredients:\n",
    "        single_word_ingredients.add(core)\n",
    "    \n",
    "    # Append the tokenized form as a tuple (core, descriptor(s)) to keep structure\n",
    "    tokenized_multi_word.append((core, descriptor))\n",
    "\n",
    "# Display the results\n",
    "print(\"Single-word Ingredients (Tokenized):\", tokenized_single_word)\n",
    "print(\"Multi-word Ingredients (Tokenized as Core + Descriptor):\", tokenized_multi_word)\n",
    "print(f\"Total unique ingredients: {len(ingredient_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26aad49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
